"""
èšç±»æœåŠ¡æ¨¡å—
æä¾›è‡ªåŠ¨èšç±»ã€é™ç»´ã€ç°‡ç®¡ç†å’Œå¯è§†åŒ–åŠŸèƒ½
"""

import sys
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import numpy as np

# ç¡®ä¿èƒ½å¯¼å…¥é¡¹ç›®æ¨¡å—
current_file_path = Path(__file__).resolve()
project_root = current_file_path.parent.parent
sys.path.append(str(project_root))

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN

# å°è¯•å¯¼å…¥ hdbscanï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ DBSCAN ä½œä¸ºå›é€€
try:
    import hdbscan
    HDBSCAN_AVAILABLE = True
except ImportError:
    HDBSCAN_AVAILABLE = False

from core.qdrant import qdrant_manager
from core.llm import get_critic_llm
from utils.logger import logger


class ClusteringService:
    """çŸ¥è¯†èšç±»æœåŠ¡ - æä¾›è‡ªåŠ¨èšç±»å’Œäº¤äº’å¼ç°‡ç®¡ç†åŠŸèƒ½"""

    def __init__(self):
        self.client = qdrant_manager.client
        self.collection_name = qdrant_manager.collection_name
        self._cache: Dict[str, Any] = {}

    def fetch_all_papers(self, limit: int = 1000) -> List[Dict[str, Any]]:
        """
        ä» Qdrant è·å–æ‰€æœ‰è®ºæ–‡çš„å‘é‡å’Œå…ƒæ•°æ®
        
        :param limit: æœ€å¤§è·å–æ•°é‡
        :return: åŒ…å« id, vector, metadata çš„è®ºæ–‡åˆ—è¡¨
        """
        try:
            # ä½¿ç”¨ scroll è·å–æ‰€æœ‰æ•°æ®
            papers = []
            offset = None
            
            while True:
                result = self.client.scroll(
                    collection_name=self.collection_name,
                    limit=100,
                    offset=offset,
                    with_vectors=True,
                    with_payload=True
                )
                
                points, offset = result
                
                if not points:
                    break
                
                for point in points:
                    payload = point.payload or {}
                    # å…¼å®¹ LangChain çš„ metadata åµŒå¥—ç»“æ„
                    meta = payload.get("metadata", payload)
                    
                    papers.append({
                        "id": point.id,
                        "vector": point.vector,
                        "metadata": meta
                    })
                
                if len(papers) >= limit or offset is None:
                    break
            
            logger.info(f"ğŸ“š Fetched {len(papers)} papers from Qdrant")
            return papers
            
        except Exception as e:
            logger.error(f"âŒ Failed to fetch papers: {e}")
            raise

    def reduce_dimensions(
        self, 
        vectors: np.ndarray, 
        n_components: int = 50,
        for_visualization: bool = False
    ) -> np.ndarray:
        """
        ä½¿ç”¨ PCA è¿›è¡Œé™ç»´
        
        :param vectors: åŸå§‹å‘é‡çŸ©é˜µ (n_samples, n_features)
        :param n_components: ç›®æ ‡ç»´åº¦
        :param for_visualization: å¦‚æœä¸º Trueï¼Œåˆ™é™åˆ° 2D/3D
        :return: é™ç»´åçš„å‘é‡
        """
        if for_visualization:
            n_components = min(3, n_components)
        
        # ç¡®ä¿ n_components ä¸è¶…è¿‡æ ·æœ¬æ•°å’Œç‰¹å¾æ•°
        n_samples, n_features = vectors.shape
        n_components = min(n_components, n_samples, n_features)
        
        pca = PCA(n_components=n_components)
        reduced = pca.fit_transform(vectors)
        
        explained_var = sum(pca.explained_variance_ratio_) * 100
        logger.info(f"ğŸ“‰ PCA: {n_features}D -> {n_components}D (explained variance: {explained_var:.1f}%)")
        
        return reduced

    def auto_cluster_hdbscan(
        self, 
        vectors: np.ndarray,
        min_cluster_size: int = 3,
        min_samples: int = 2,
        eps: float = 0.5
    ) -> Tuple[np.ndarray, int]:
        """
        ä½¿ç”¨ HDBSCAN è¿›è¡Œè‡ªåŠ¨èšç±»ï¼ˆå¦‚æœä¸å¯ç”¨åˆ™å›é€€åˆ° DBSCANï¼‰
        
        :param vectors: å‘é‡çŸ©é˜µ
        :param min_cluster_size: æœ€å°ç°‡å¤§å°
        :param min_samples: æ ¸å¿ƒæ ·æœ¬æœ€å°é‚»å±…æ•°
        :param eps: DBSCAN çš„é‚»åŸŸåŠå¾„ï¼ˆä»…åœ¨ä½¿ç”¨ DBSCAN æ—¶ç”Ÿæ•ˆï¼‰
        :return: (ç°‡æ ‡ç­¾æ•°ç»„, ç°‡æ•°é‡)
        """
        if HDBSCAN_AVAILABLE:
            clusterer = hdbscan.HDBSCAN(
                min_cluster_size=min_cluster_size,
                min_samples=min_samples,
                metric='euclidean',
                cluster_selection_method='eom'
            )
            labels = clusterer.fit_predict(vectors)
            algo_name = "HDBSCAN"
        else:
            # å›é€€åˆ° DBSCAN
            clusterer = DBSCAN(
                eps=eps,
                min_samples=min_samples,
                metric='euclidean'
            )
            labels = clusterer.fit_predict(vectors)
            algo_name = "DBSCAN (fallback)"
        
        # ç»Ÿè®¡ç°‡æ•°é‡ (-1 è¡¨ç¤ºå™ªå£°ç‚¹)
        unique_labels = set(labels)
        n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
        n_noise = list(labels).count(-1)
        
        logger.info(f"ğŸ§¬ {algo_name}: Found {n_clusters} clusters, {n_noise} noise points")
        
        return labels, n_clusters

    def auto_cluster_kmeans(
        self, 
        vectors: np.ndarray,
        n_clusters: int = 5
    ) -> Tuple[np.ndarray, int]:
        """
        ä½¿ç”¨ K-Means è¿›è¡Œèšç±»ï¼ˆéœ€æŒ‡å®šç°‡æ•°é‡ï¼‰
        
        :param vectors: å‘é‡çŸ©é˜µ
        :param n_clusters: ç°‡æ•°é‡
        :return: (ç°‡æ ‡ç­¾æ•°ç»„, ç°‡æ•°é‡)
        """
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        labels = kmeans.fit_predict(vectors)
        
        logger.info(f"ğŸ§¬ K-Means: Created {n_clusters} clusters")
        
        return labels, n_clusters

    def generate_cluster_labels(
        self, 
        papers_by_cluster: Dict[int, List[Dict]],
        max_papers_per_cluster: int = 5
    ) -> Dict[int, str]:
        """
        ä½¿ç”¨ LLM ä¸ºæ¯ä¸ªç°‡ç”Ÿæˆå¸¦è¯„åˆ†çš„å…³é”®è¯æ ‡ç­¾
        
        :param papers_by_cluster: æŒ‰ç°‡åˆ†ç»„çš„è®ºæ–‡ {cluster_id: [papers]}
        :param max_papers_per_cluster: æ¯ä¸ªç°‡ç”¨äºç”Ÿæˆæ ‡ç­¾çš„æœ€å¤§è®ºæ–‡æ•°
        :return: {cluster_id: topic_label}
        """
        llm = get_critic_llm()
        cluster_labels = {}
        
        for cluster_id, papers in papers_by_cluster.items():
            if cluster_id == -1:  # è·³è¿‡å™ªå£°ç‚¹
                cluster_labels[-1] = "ğŸ”‡ Noise / Uncategorized"
                continue
            
            # å–å‰ N ç¯‡è®ºæ–‡çš„æ ‡é¢˜å’Œæ‘˜è¦
            sample_papers = papers[:max_papers_per_cluster]
            paper_info = "\n".join([
                f"- Title: {p['metadata'].get('title', 'Unknown')}\n  Abstract: {p['metadata'].get('abstract', '')[:300]}..."
                for p in sample_papers
            ])
            
            prompt = f"""Analyze the following academic papers and generate keyword tags with relevance scores.

Papers in this cluster ({len(papers)} total):
{paper_info}

Task:
1. Identify 3-5 keywords that best describe the common theme of these papers
2. Score each keyword from 0.0 to 1.0 based on how well it represents ALL papers in this cluster
3. Format: keyword1 (score), keyword2 (score), keyword3 (score)

Example output:
Federated Learning (0.95), Privacy Preservation (0.82), Gradient Compression (0.71)

Requirements:
- Each keyword should be 1-3 words
- Scores should reflect relevance: 0.9+ = core theme, 0.7-0.9 = important, 0.5-0.7 = related
- Use English academic terminology
- Output ONLY the formatted keywords with scores, nothing else

Keywords with scores:"""

            try:
                response = llm.invoke(prompt)
                label = response.content.strip().strip('"\'')
                cluster_labels[cluster_id] = label
                logger.info(f"ğŸ·ï¸ Cluster {cluster_id}: {label}")
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to generate label for cluster {cluster_id}: {e}")
                cluster_labels[cluster_id] = f"Cluster {cluster_id}"
        
        return cluster_labels

    def merge_clusters(
        self, 
        labels: np.ndarray, 
        cluster_ids_to_merge: List[int]
    ) -> np.ndarray:
        """
        åˆå¹¶æŒ‡å®šçš„ç°‡
        
        :param labels: å½“å‰ç°‡æ ‡ç­¾
        :param cluster_ids_to_merge: è¦åˆå¹¶çš„ç°‡ ID åˆ—è¡¨
        :return: æ›´æ–°åçš„æ ‡ç­¾
        """
        if len(cluster_ids_to_merge) < 2:
            return labels
        
        new_labels = labels.copy()
        target_cluster = min(cluster_ids_to_merge)  # åˆå¹¶åˆ°æœ€å° ID
        
        for cluster_id in cluster_ids_to_merge:
            new_labels[labels == cluster_id] = target_cluster
        
        logger.info(f"ğŸ”— Merged clusters {cluster_ids_to_merge} -> {target_cluster}")
        return new_labels

    def split_cluster(
        self, 
        vectors: np.ndarray, 
        labels: np.ndarray, 
        cluster_id: int,
        n_splits: int = 2
    ) -> np.ndarray:
        """
        å°†æŒ‡å®šç°‡æ‹†åˆ†ä¸ºå¤šä¸ªå­ç°‡
        
        :param vectors: åŸå§‹å‘é‡
        :param labels: å½“å‰ç°‡æ ‡ç­¾
        :param cluster_id: è¦æ‹†åˆ†çš„ç°‡ ID
        :param n_splits: æ‹†åˆ†æ•°é‡
        :return: æ›´æ–°åçš„æ ‡ç­¾
        """
        new_labels = labels.copy()
        mask = labels == cluster_id
        
        if mask.sum() < n_splits:
            logger.warning(f"âš ï¸ Cluster {cluster_id} has too few points to split")
            return labels
        
        # å¯¹è¯¥ç°‡å†…çš„ç‚¹è¿›è¡Œ K-Means
        cluster_vectors = vectors[mask]
        kmeans = KMeans(n_clusters=n_splits, random_state=42, n_init=10)
        sub_labels = kmeans.fit_predict(cluster_vectors)
        
        # åˆ†é…æ–°çš„ç°‡ ID
        max_label = labels.max()
        new_cluster_ids = [cluster_id] + [max_label + i + 1 for i in range(n_splits - 1)]
        
        indices = np.where(mask)[0]
        for i, idx in enumerate(indices):
            new_labels[idx] = new_cluster_ids[sub_labels[i]]
        
        logger.info(f"âœ‚ï¸ Split cluster {cluster_id} into {new_cluster_ids}")
        return new_labels

    def group_papers_by_cluster(
        self, 
        papers: List[Dict], 
        labels: np.ndarray
    ) -> Dict[int, List[Dict]]:
        """
        æŒ‰ç°‡åˆ†ç»„è®ºæ–‡
        
        :param papers: è®ºæ–‡åˆ—è¡¨
        :param labels: ç°‡æ ‡ç­¾
        :return: {cluster_id: [papers]}
        """
        grouped = {}
        for i, paper in enumerate(papers):
            label = int(labels[i])
            if label not in grouped:
                grouped[label] = []
            grouped[label].append(paper)
        
        return grouped

    def prepare_visualization_data(
        self, 
        papers: List[Dict], 
        labels: np.ndarray,
        cluster_names: Dict[int, str],
        n_dims: int = 2
    ) -> Dict[str, Any]:
        """
        å‡†å¤‡å¯è§†åŒ–æ•°æ®
        
        :param papers: è®ºæ–‡åˆ—è¡¨
        :param labels: ç°‡æ ‡ç­¾
        :param cluster_names: ç°‡åç§°æ˜ å°„
        :param n_dims: å¯è§†åŒ–ç»´åº¦ (2 æˆ– 3)
        :return: å¯ç”¨äº Plotly çš„æ•°æ®ç»“æ„
        """
        vectors = np.array([p["vector"] for p in papers])
        
        # é™ç»´åˆ° 2D æˆ– 3D
        reduced = self.reduce_dimensions(vectors, n_components=n_dims, for_visualization=True)
        
        viz_data = {
            "x": reduced[:, 0].tolist(),
            "y": reduced[:, 1].tolist(),
            "z": reduced[:, 2].tolist() if n_dims >= 3 else None,
            "labels": labels.tolist(),
            "cluster_names": [cluster_names.get(int(l), f"Cluster {l}") for l in labels],
            "titles": [p["metadata"].get("title", "Unknown") for p in papers],
            "ids": [p["id"] for p in papers]
        }
        
        return viz_data


# å•ä¾‹å®ä¾‹
clustering_service = ClusteringService()


if __name__ == "__main__":
    # æµ‹è¯•è„šæœ¬
    print("-" * 50)
    print("ğŸ§¬ Testing Clustering Service...")
    
    try:
        # 1. è·å–è®ºæ–‡
        papers = clustering_service.fetch_all_papers(limit=50)
        print(f"âœ… Fetched {len(papers)} papers")
        
        if len(papers) < 3:
            print("âš ï¸ Not enough papers for clustering test")
        else:
            # 2. å‡†å¤‡å‘é‡
            vectors = np.array([p["vector"] for p in papers])
            print(f"ğŸ“ Vector shape: {vectors.shape}")
            
            # 3. é™ç»´
            reduced = clustering_service.reduce_dimensions(vectors, n_components=50)
            print(f"ğŸ“‰ Reduced shape: {reduced.shape}")
            
            # 4. èšç±»
            labels, n_clusters = clustering_service.auto_cluster_hdbscan(
                reduced, 
                min_cluster_size=2,
                min_samples=1
            )
            print(f"ğŸ§¬ Found {n_clusters} clusters")
            
            # 5. åˆ†ç»„
            grouped = clustering_service.group_papers_by_cluster(papers, labels)
            for cluster_id, cluster_papers in grouped.items():
                print(f"   Cluster {cluster_id}: {len(cluster_papers)} papers")
        
        print("âœ… Clustering Service is Ready!")
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()

import streamlit as st
import numpy as np
from typing import List, Dict, Any

# --- å¯¼å…¥æ ¸å¿ƒæ¨¡å— ---
from core.qdrant import qdrant_manager
from core.llm import get_embeddings
from utils.logger import logger

st.set_page_config(page_title="Topic Clustering", page_icon="ğŸ§¬")

st.title("ğŸ§¬ Knowledge Clustering")
st.caption("å¤šç»´è¯­ä¹‰åˆ†æï¼šåˆ†æè®ºæ–‡ä¸å¤šä¸ªç ”ç©¶ä¸»é¢˜çš„å…³è”å¼ºåº¦")

# ==========================================
# 1. Session State (å…³é”®è¯ç®¡ç†)
# ==========================================
if "cluster_keywords" not in st.session_state:
    st.session_state.cluster_keywords = [""]

def add_keyword():
    st.session_state.cluster_keywords.append("")

def remove_keyword(index):
    if len(st.session_state.cluster_keywords) > 0:
        st.session_state.cluster_keywords.pop(index)

# ==========================================
# 2. ä¾§è¾¹æ ï¼šé…ç½®
# ==========================================
with st.sidebar:
    st.header("âš™ï¸ Analysis Config")
    
    top_k = st.slider("Max Papers per Topic", 1, 20, 5, help="æ¯ä¸ªå…³é”®è¯æœ€å¤šæ‰¾å‡ºå‡ ç¯‡è®ºæ–‡ï¼Ÿ")
    score_threshold = st.slider("Min Similarity", 0.0, 1.0, 0.2, help="è¿‡æ»¤æ‰ä¸ç›¸å…³çš„è®ºæ–‡")
    
    st.divider()
    st.markdown("### ğŸ•µï¸â€â™‚ï¸ Strict Mode")
    strict_match = st.checkbox("Require Keyword Match", value=False, 
                               help="é€‰ä¸­åï¼Œè®ºæ–‡å¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªå…³é”®è¯çš„æ–‡æœ¬")

# ==========================================
# 3. è¾…åŠ©å‡½æ•°ï¼šä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
# ==========================================
def cosine_similarity(v1: List[float], v2: List[float]) -> float:
    """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
    a = np.array(v1)
    b = np.array(v2)
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)
    if norm_a == 0 or norm_b == 0:
        return 0.0
    return np.dot(a, b) / (norm_a * norm_b)

# ==========================================
# 4. ä¸»ç•Œé¢ï¼šå®šä¹‰å…³é”®è¯
# ==========================================
st.subheader("1. Define Research Topics")
st.info("è¾“å…¥å¤šä¸ªå…³é”®è¯ï¼ˆå¦‚ 'RAG', 'Agent', 'Evaluation'ï¼‰ï¼Œæˆ‘ä»¬å°†åˆ†æè®ºæ–‡åœ¨è¿™äº›ç»´åº¦ä¸Šçš„å¾—åˆ†ã€‚")

# æ¸²æŸ“è¾“å…¥æ¡†
for i, keyword in enumerate(st.session_state.cluster_keywords):
    col1, col2 = st.columns([5, 1])
    with col1:
        st.session_state.cluster_keywords[i] = st.text_input(
            f"Topic #{i+1}", 
            value=keyword, 
            key=f"kw_{i}",
            placeholder="Enter keyword..."
        )
    with col2:
        if st.button("ğŸ—‘ï¸", key=f"del_{i}"):
            remove_keyword(i)
            st.rerun()

if st.button("â• Add Topic"):
    add_keyword()
    st.rerun()

st.divider()

# ==========================================
# 5. æ‰§è¡Œåˆ†æ (èšåˆå»é‡ + å…¨é‡æ‰“åˆ†)
# ==========================================
st.subheader("2. Consolidated Results")

if st.button("ğŸš€ Start Multi-Topic Analysis", type="primary"):
    valid_keywords = [k.strip() for k in st.session_state.cluster_keywords if k.strip()]
    
    if not valid_keywords:
        st.warning("âš ï¸ Please enter at least one keyword.")
    else:
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        try:
            embedding_model = get_embeddings()
            client = qdrant_manager.client
            collection_name = qdrant_manager.collection_name
            
            # --- ç¬¬ä¸€æ­¥ï¼šé¢„è®¡ç®—æ‰€æœ‰å…³é”®è¯çš„å‘é‡ ---
            status_text.text("ğŸ§  Embedding keywords...")
            keyword_vectors = {}
            for kw in valid_keywords:
                keyword_vectors[kw] = embedding_model.embed_query(kw)
            
            # --- ç¬¬äºŒæ­¥ï¼šæœç´¢å€™é€‰è®ºæ–‡ (èšåˆ) ---
            candidate_papers = {} # Map[id, {metadata, vector}]
            
            for idx, (kw, kw_vec) in enumerate(keyword_vectors.items()):
                status_text.text(f"ğŸ” Searching candidates for '{kw}'...")
                
                # æ³¨æ„ï¼šå¿…é¡»å¼€å¯ with_vectors=True æ‰èƒ½åœ¨æœ¬åœ°è¿›è¡Œå¤šç»´æ‰“åˆ†
                search_result = client.query_points(
                    collection_name=collection_name,
                    query=kw_vec,
                    limit=top_k,
                    score_threshold=score_threshold,
                    with_payload=True,
                    with_vectors=True 
                )
                
                hits = getattr(search_result, 'points', search_result)
                
                for hit in hits:
                    # ä½¿ç”¨ payload.get("source") æˆ– hit.id ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    paper_id = hit.id 
                    
                    if paper_id not in candidate_papers:
                        payload = hit.payload or {}
                        # å…¼å®¹ LangChain çš„ metadata åµŒå¥—ç»“æ„
                        meta = payload.get("metadata", payload)
                        
                        candidate_papers[paper_id] = {
                            "metadata": meta,
                            "vector": hit.vector, # è·å–å‘é‡
                            "source_id": hit.id
                        }
                
                progress_bar.progress((idx + 1) / (len(valid_keywords) + 1))

            # --- ç¬¬ä¸‰æ­¥ï¼šäº¤å‰æ‰“åˆ†ä¸è¿‡æ»¤ ---
            status_text.text("ğŸ“Š Calculating cross-topic scores...")
            final_results = []
            
            for pid, data in candidate_papers.items():
                meta = data["metadata"]
                paper_vec = data["vector"]
                
                # 1. ä¸¥æ ¼æ¨¡å¼æ£€æŸ¥ (æ–‡æœ¬åŒ¹é…)
                if strict_match:
                    combined_text = (
                        meta.get("title", "") + 
                        meta.get("abstract", "") + 
                        meta.get("introduction_summary", "")
                    ).lower()
                    
                    # åªè¦åŒ…å«ä»»æ„ä¸€ä¸ªå…³é”®è¯å³å¯ä¿ç•™ (æˆ–è€…ä½ å¯ä»¥æ”¹ä¸ºå¿…é¡»åŒ…å«æ‰€æœ‰)
                    has_match = any(kw.lower() in combined_text for kw in valid_keywords)
                    if not has_match:
                        continue

                # 2. è®¡ç®—è¯¥è®ºæ–‡å¯¹ *æ‰€æœ‰* å…³é”®è¯çš„å¾—åˆ†
                scores = {}
                total_score = 0
                for kw, kw_vec in keyword_vectors.items():
                    # å¦‚æœ paper_vec æ˜¯ None (æŸäº›æ—§æ•°æ®å¯èƒ½æ²¡å­˜å‘é‡)ï¼Œåˆ™æ— æ³•è®¡ç®—
                    if paper_vec is None:
                        sim = 0.0
                    else:
                        sim = cosine_similarity(kw_vec, paper_vec)
                    
                    scores[kw] = sim
                    total_score += sim
                
                # å­˜å…¥ç»“æœå¯¹è±¡
                final_results.append({
                    "metadata": meta,
                    "scores": scores,
                    "avg_score": total_score / len(valid_keywords),
                    "max_score": max(scores.values()) if scores else 0
                })

            # --- ç¬¬å››æ­¥ï¼šæ’åºä¸å±•ç¤º ---
            # æŒ‰æœ€é«˜åŒ¹é…åˆ†æ’åº
            final_results.sort(key=lambda x: x["max_score"], reverse=True)

            # ã€æ–°å¢ã€‘å¦‚æœä½ æƒ³å¼ºåˆ¶é™åˆ¶æœ€ç»ˆå±•ç¤ºçš„æ€»æ•°é‡ï¼ˆä¾‹å¦‚åªçœ‹å…¨åœºæœ€ä½³çš„ 5 ç¯‡ï¼‰
            # final_results = final_results[:5]  <-- å–æ¶ˆæ³¨é‡Šè¿™è¡Œå³å¯æˆªæ–­
            
            progress_bar.empty()
            status_text.empty()
            
            st.success(f"âœ… Found {len(final_results)} unique papers relevant to your topics.")
            
            if not final_results:
                st.warning("No papers met the criteria.")
            
            for item in final_results:
                meta = item["metadata"]
                scores = item["scores"]
                
                title = meta.get("title", "Unknown Title")
                venue = meta.get("venue", "Unknown Venue")
                year = meta.get("year", "N/A")
                authors = meta.get("authors", [])
                if isinstance(authors, list):
                    authors_str = ", ".join(authors[:3]) + ("..." if len(authors) > 3 else "")
                else:
                    authors_str = str(authors)
                
                # å¤–å±‚å®¹å™¨
                with st.container(border=True):
                    # æ ‡é¢˜è¡Œ
                    st.markdown(f"### ğŸ“„ {title}")
                    st.caption(f"ğŸ‘¤ {authors_str} | ğŸ›ï¸ {venue} ({year})")
                    
                    st.divider()
                    
                    # å…³é”®è¯å¾—åˆ†å±•ç¤ºåŒº (Grid Layout)
                    st.markdown("**Topic Relevance:**")
                    
                    # åŠ¨æ€åˆ—å¸ƒå±€ï¼šæ¯è¡Œæ˜¾ç¤º 4 ä¸ªå¾—åˆ†
                    cols = st.columns(4)
                    for i, (kw, score) in enumerate(scores.items()):
                        col = cols[i % 4]
                        # æ ¹æ®åˆ†æ•°é«˜ä½æ˜¾ç¤ºä¸åŒé¢œè‰²
                        if score > 0.75:
                            color = "green"
                        elif score > 0.5:
                            color = "orange"
                        else:
                            color = "gray"
                            
                        col.markdown(f"**{kw}**: :{color}[`{score:.4f}`]")

        except Exception as e:
            st.error(f"âŒ Analysis failed: {str(e)}")
            logger.error(f"Clustering Error: {e}")
            if "with_vectors" in str(e):
                st.info("ğŸ’¡ Hint: Ensure your Qdrant instance allows retrieving vectors.")
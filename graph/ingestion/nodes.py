import json
import yaml
from pathlib import Path
from typing import Dict, Any

from langchain_core.messages import SystemMessage, HumanMessage
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

from config.settings import settings
from core.llm import get_extractor_llm, get_embeddings
from core.pdf_loader import load_pdf_as_images
from core.qdrant import qdrant_manager
from core.search import search_tool
from graph.ingestion.state import IngestionState
from utils.logger import logger

# --- è¾…åŠ©å‡½æ•°: åŠ è½½ Prompt ---
def load_prompts():
    prompt_path = settings.PROMPTS_DIR / "ingestion.yaml"
    with open(prompt_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

PROMPTS = load_prompts()

# ==========================================
# Node 1: å…ƒæ•°æ®æå–èŠ‚ç‚¹
# ==========================================
def extract_metadata_node(state: IngestionState) -> Dict[str, Any]:
    logger.info(f"ğŸ‘ï¸ Processing Node: Visual Extraction for {state['pdf_path']}")
    
    # 1. åŠ è½½å›¾ç‰‡ (å¦‚æœ state é‡Œæ²¡æœ‰)
    images = state.get("page_images")
    if not images:
        # è°ƒç”¨æ–°çš„å›¾ç‰‡åŠ è½½å™¨
        images = load_pdf_as_images(state["pdf_path"], max_pages=5)
    
    # 2. å‡†å¤‡è§†è§‰æ¨¡å‹çš„è¾“å…¥
    llm = get_extractor_llm()
    prompt_cfg = PROMPTS["extract_metadata"]
    
    # --- æ„é€ å¤šæ¨¡æ€æ¶ˆæ¯ ---
    # User æ¶ˆæ¯åŒ…å«ä¸¤éƒ¨åˆ†ï¼šæ–‡æœ¬æŒ‡ä»¤ + å›¾ç‰‡åˆ—è¡¨
    user_content = [
        {"type": "text", "text": prompt_cfg["user"]} # è¿™é‡Œä¸éœ€è¦å† format {text} äº†
    ]
    
    # æŠŠ 5 å¼ å›¾ç‰‡ä¾æ¬¡åŠ è¿›å»
    for img_b64 in images:
        user_content.append({
            "type": "image_url",
            "image_url": {
                # å‘Šè¯‰æ¨¡å‹è¿™æ˜¯ JPEG/PNG å›¾ç‰‡
                "url": f"data:image/png;base64,{img_b64}"
            }
        })
        
    messages = [
        SystemMessage(content=prompt_cfg["system"]),
        HumanMessage(content=user_content) # LangChain ä¼šè‡ªåŠ¨å¤„ç†è¿™ä¸ªåˆ—è¡¨
    ]
    
    # 3. è°ƒç”¨æ¨¡å‹ (åé¢çš„é€»è¾‘å’Œä¹‹å‰ä¸€æ ·)
    try:
        logger.info("   ğŸ“¤ Sending images to Vision LLM...")
        response = llm.invoke(messages)
        content = response.content.replace("```json", "").replace("```", "").strip()
        metadata = json.loads(content)
        
        logger.info(f"   âœ… Visual Extraction Success: {metadata.get('title')}")

        # 4. å…³é”®ï¼šAgent è‡ªæˆ‘æ£€æŸ¥ (Reflection)
        missing = []
        if not metadata.get("year"): missing.append("year")
        venue = metadata.get("venue", "").lower()
        if not venue or "arxiv" in venue or "preprint" in venue: missing.append("venue")
        
        if missing:
            logger.warning(f"   âš ï¸ Missing/Incomplete fields (triggering search): {missing}")
        
        return {
            "page_images": images,
            "metadata": metadata,
            "missing_fields": missing,
            "retry_count": state.get("retry_count", 0)
        }
        
    except json.JSONDecodeError:
        logger.error("âŒ Failed to parse JSON from LLM")
        return {"status": "failed", "error_msg": "JSON Parse Error"}
    except Exception as e:
        logger.error(f"âŒ Extraction Error: {e}")
        return {"status": "failed", "error_msg": str(e)}

# ==========================================
# Node 2: è”ç½‘ä¿®å¤èŠ‚ç‚¹ (Agentic Loop)
# ==========================================
def web_fixer_node(state: IngestionState) -> Dict[str, Any]:
    """
    ç”Ÿæˆæœç´¢è¯ -> è”ç½‘ -> ä¿®æ­£ Metadata
    """
    current_retries = state.get("retry_count", 0)
    logger.info(f"ğŸŒ Processing Node: Web Search Fixer (Attempt {current_retries + 1})")
    
    metadata = state["metadata"]
    missing = state["missing_fields"]
    
    # 1. ç”Ÿæˆæœç´¢å…³é”®è¯ (ç®€å•èµ·è§ï¼Œç›´æ¥ç”¨ Python æ‹¼æ¥ï¼Œä¹Ÿå¯ä»¥ç”¨ LLM ç”Ÿæˆ)
    # PROMPTS["generate_search_query"] å¯ä»¥åœ¨è¿™é‡Œç”¨ï¼Œä½†ä¸ºäº†çœ Tokenï¼Œç›´æ¥æ‹¼ä¹Ÿä¸é”™ï¼š
    query = f"{metadata['title']} paper conference year bibtex"
    
    # 2. æ‰§è¡Œæœç´¢
    search_results = search_tool.search(query)
    
    # 3. è°ƒç”¨ llm æ ¹æ®æœç´¢ç»“æœä¿®å¤
    llm = get_extractor_llm()
    prompt_cfg = PROMPTS["fix_metadata"]
    
    messages = [
        SystemMessage(content=prompt_cfg["system"].format(
            current_venue=metadata.get("venue", "Unknown") # ğŸ‘ˆ æ³¨å…¥å½“å‰ venue
        )),
        HumanMessage(content=prompt_cfg["user"].format(
            title=metadata['title'],
            current_venue=metadata.get("venue", "Unknown"),
            missing_fields=missing,
            search_results=search_results
        ))
    ]
    
    # 4. æ›´æ–° Metadata
    try:
        response = llm.invoke(messages)
        fix_json = json.loads(response.content.replace("```json", "").replace("```", "").strip())
        
        # åˆå¹¶æ–°æ—§æ•°æ®
        if fix_json:
            metadata.update(fix_json)
            logger.info(f"   âœ… Fixed Metadata: {fix_json}")
        else:
            logger.info("   âŒ Could not find info from web.")
            
    except Exception as e:
        logger.error(f"   Web fix failed: {e}")
    
    # 5. å†æ¬¡æ£€æŸ¥æ˜¯å¦è¿˜ç¼ºå­—æ®µ (å†³å®šæ˜¯å¦ç»§ç»­ Loop)
    new_missing = []
    if not metadata.get("year"): new_missing.append("year")
    if not metadata.get("venue"): new_missing.append("venue")
    
    return {
        "metadata": metadata,
        "missing_fields": new_missing,
        "retry_count": current_retries + 1
    }

# ==========================================
# Node 3: å‘é‡å…¥åº“èŠ‚ç‚¹
# ==========================================
def ingest_to_qdrant_node(state: IngestionState) -> Dict[str, Any]:
    logger.info("ğŸ’¾ Processing Node: Ingest High-Quality Metadata to Qdrant")
    
    metadata = state["metadata"]
    
    # 1. æ„é€ åˆæˆæ–‡æ¡£ (ä¿æŒä¸å˜)
    content_parts = [
        f"Title: {metadata.get('title', 'Unknown')}",
        f"Year: {metadata.get('year', 'Unknown')}",
        f"Venue: {metadata.get('venue', 'Unknown')}", # è¿™é‡Œçš„ venue åº”è¯¥æ˜¯ä¿®æ­£åçš„
        f"Authors: {', '.join(metadata.get('authors', []))}",
        "--- Abstract ---",
        metadata.get('abstract', 'No abstract extracted.'),
        "--- Core Introduction & Background ---",
        metadata.get('introduction_summary', 'No summary provided.')
    ]
    clean_text = "\n\n".join(content_parts)
    
    # 2. ğŸŒŸ æ ¸å¿ƒä¿®æ”¹ï¼šå–æ¶ˆåˆ‡ç‰‡ï¼Œç›´æ¥å°è£…æˆä¸€ä¸ª Document
    # ä¹‹å‰çš„ RecursiveCharacterTextSplitter æŠŠè¿™ä¸ª clean_text åˆ‡æˆäº†å‡ æ®µ
    # å¯¼è‡´æ•°æ®åº“é‡Œå‡ºç°äº†å¤šæ¡æ‹¥æœ‰ç›¸åŒ Metadata çš„è®°å½•
    final_doc = Document(
        page_content=clean_text,
        metadata={
            **metadata,
            "source": str(state["pdf_path"]),
            "content_type": "ai_generated_summary"
        }
    )
    
    # 3. å†™å…¥ Qdrant
    try:
        qdrant_manager.ensure_collection_exists()
        client = qdrant_manager.client
        embedding_model = get_embeddings()
        
        from langchain_qdrant import QdrantVectorStore
        
        vector_store = QdrantVectorStore(
            client=client,
            collection_name=settings.QDRANT_COLLECTION_NAME,
            embedding=embedding_model
        )
        
        # ç›´æ¥æ·»åŠ è¿™ä¸€ä¸ªæ–‡æ¡£
        vector_store.add_documents([final_doc]) 
        logger.info(f"   âœ… Successfully ingested 1 single document (Length: {len(clean_text)}).")
        
        return {"status": "success"}
        
    except Exception as e:
        logger.error(f"âŒ Database Error: {e}")
        return {"status": "failed", "error_msg": str(e)}
import json
import yaml
from typing import Dict, Any, List

from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.documents import Document
from langchain_qdrant import QdrantVectorStore

from config.settings import settings
from core.llm import get_agent_llm, get_embeddings
from core.qdrant import qdrant_manager
from core.search import search_tool
from graph.research.state import ResearchState
from utils.logger import logger

# --- è¾…åŠ©å‡½æ•°: åŠ è½½ Research Prompt ---
def load_prompts():
    prompt_path = settings.PROMPTS_DIR / "research.yaml"
    with open(prompt_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

PROMPTS = load_prompts()

# ==========================================
# Node 1: æ„å›¾è·¯ç”±èŠ‚ç‚¹ (Router)
# ==========================================
def router_node(state: ResearchState) -> Dict[str, Any]:
    """
    åˆ†æç”¨æˆ·æ„å›¾ï¼šæ˜¯åªæŸ¥æœ¬åœ°çŸ¥è¯†åº“ï¼Œè¿˜æ˜¯éœ€è¦è”ç½‘ï¼Ÿ
    """
    logger.info("ğŸš¦ Processing Node: Router")
    question = state["question"]
    
    llm = get_agent_llm(temperature=0) # å†³ç­–éœ€è¦ç¨³å®š
    prompt_cfg = PROMPTS["router"]
    
    messages = [
        SystemMessage(content=prompt_cfg["system"]),
        HumanMessage(content=prompt_cfg["user"].format(question=question))
    ]
    
    try:
        response = llm.invoke(messages)
        content = response.content.replace("```json", "").replace("```", "").strip()
        decision_json = json.loads(content)
        decision = decision_json.get("decision", "web_search") # é»˜è®¤è”ç½‘ï¼Œæ¯”è¾ƒç¨³å¦¥
        
        logger.info(f"   ğŸ‘‰ Decision: {decision}")
        return {"router_decision": decision}
        
    except Exception as e:
        logger.error(f"âŒ Router failed: {e}. Fallback to web_search.")
        return {"router_decision": "web_search"}

# ==========================================
# Node 2: æœ¬åœ°æ£€ç´¢èŠ‚ç‚¹ (Retriever)
# ==========================================
def retrieve_node(state: ResearchState) -> Dict[str, Any]:
    """
    ä» Qdrant æ£€ç´¢ç›¸å…³æ–‡æ¡£
    """
    logger.info("ğŸ” Processing Node: Local Retriever")
    question = state["question"]
    
    try:
        client = qdrant_manager.client
        embedding_model = get_embeddings()
        
        vector_store = QdrantVectorStore(
            client=client,
            collection_name=settings.QDRANT_COLLECTION_NAME,
            embedding=embedding_model
        )
        
        # æ£€ç´¢ Top 5 (æ ¹æ®ä¹‹å‰çš„åˆæˆæ–‡æ¡£ï¼Œè¿™é‡Œæ£€ç´¢åˆ°çš„å·²ç»æ˜¯é«˜è´¨é‡ Summary äº†)
        docs = vector_store.similarity_search(question, k=5)
        logger.info(f"   âœ… Retrieved {len(docs)} documents from Qdrant.")
        
        return {"context": docs} # å°†ç»“æœå­˜å…¥ context
        
    except Exception as e:
        logger.error(f"âŒ Retrieval failed: {e}")
        return {"context": []}

# ==========================================
# Node 3: è”ç½‘æœç´¢èŠ‚ç‚¹ (Web Search)
# ==========================================
def web_search_node(state: ResearchState) -> Dict[str, Any]:
    """
    ç”Ÿæˆå…³é”®è¯ -> è”ç½‘æœç´¢ -> å°è£…ä¸º Document
    """
    logger.info("ğŸŒ Processing Node: Web Search")
    question = state["question"]
    existing_context = state.get("context", [])
    
    llm = get_agent_llm()
    
    # 1. ç”Ÿæˆæœç´¢è¯
    prompt_cfg = PROMPTS["generate_search_query"]
    messages = [
        SystemMessage(content=prompt_cfg["system"]),
        HumanMessage(content=prompt_cfg["user"].format(question=question))
    ]
    query_res = llm.invoke(messages).content.strip()
    # ç®€å•å¤„ç†ï¼šå‡è®¾ LLM è¿”å›çš„æ˜¯é€—å·åˆ†éš”çš„å…³é”®è¯
    queries = [q.strip() for q in query_res.split(",")]
    
    logger.info(f"   ğŸ” Generated Queries: {queries}")
    
    # 2. æ‰§è¡Œæœç´¢ (åªæœç¬¬ä¸€ä¸ªè¯ï¼Œæˆ–è€…å¹¶å‘æœ)
    # ä¸ºäº†æ¼”ç¤ºç®€å•ï¼Œæˆ‘ä»¬åªç”¨ç¬¬ä¸€ä¸ªå…³é”®è¯å»æœ
    search_query = queries[0]
    search_result_str = search_tool.search(search_query)
    
    # 3. å°†æœç´¢ç»“æœå°è£…æˆ Document å¯¹è±¡ï¼Œä»¥ä¾¿å’Œ Qdrant ç»“æœæ ¼å¼ç»Ÿä¸€
    web_doc = Document(
        page_content=search_result_str,
        metadata={"source": "web_search", "query": search_query}
    )
    
    # 4. è¿½åŠ åˆ°ç°æœ‰ Context
    return {
        "context": existing_context + [web_doc], # åˆå¹¶
        "search_queries": queries
    }

# ==========================================
# Node 4: ç»¼è¿°æ’°å†™èŠ‚ç‚¹ (Writer)
# ==========================================
def writer_node(state: ResearchState) -> Dict[str, Any]:
    """
    è¯»å– Context -> ç”Ÿæˆæœ€ç»ˆå›ç­”
    """
    logger.info("âœï¸ Processing Node: Writer")
    question = state["question"]
    context_docs = state.get("context", [])
    messages = state.get("messages", [])
    
    if not context_docs:
        return {"answer": "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç›¸å…³èµ„æ–™ï¼Œæ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚"}
    
    # 1. æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
    context_str = ""
    for i, doc in enumerate(context_docs):
        source = doc.metadata.get("title", "Web Search")
        venue = doc.metadata.get("venue", "")
        year = doc.metadata.get("year", "")
        context_str += f"\n--- Reference {i+1} ---\n{doc.page_content}\n"
    
    # 2. ğŸŒŸ æ ¼å¼åŒ–å†å²æ¶ˆæ¯ (æ ¸å¿ƒä¿®æ”¹)
    # æŠŠæœ€è¿‘çš„å¯¹è¯å˜æˆå­—ç¬¦ä¸²ï¼Œå–‚ç»™æ¨¡å‹
    history_str = ""
    #recent_history = messages[:-1][-10:] 
    #å–å‰©ä¸‹å†å²ä¸­çš„æœ€å 10 æ¡ (å³æœ€è¿‘ 5 è½®é—®ç­”),å¦‚æœä½ æƒ³ä¿ç•™ 10 è½®ï¼Œå°±æ”¹æˆ [-20:]
    recent_history = messages[:-1] # ä¸åŒ…å«å½“å‰æœ€æ–°çš„è¿™æ¡é—®é¢˜
    for msg in recent_history:
        role = "User" if isinstance(msg, HumanMessage) else "Assistant"
        history_str += f"{role}: {msg.content}\n"
    
    # 3. è°ƒç”¨ LLM
    llm = get_agent_llm(temperature=0.7) 
    prompt_cfg = PROMPTS["write_review"]
    
    system_msg = prompt_cfg["system"].format(
        context=context_str,
        chat_history=history_str, # ğŸ‘ˆ æ³¨å…¥å†å²
        question=question
    )
    
    msg_payload = [
        SystemMessage(content=system_msg),
        HumanMessage(content=question)
    ]
    
    try:
        response = llm.invoke(msg_payload)
        logger.info("   âœ… Answer generated.")
        
        # ğŸŒŸ å…³é”®ï¼šè¿”å› messages ä»¥ä¾¿ LangGraph è‡ªåŠ¨ä¿å­˜
        # æˆ‘ä»¬è¿”å›ä¸€ä¸ª AIMessageï¼Œadd_messages ä¼šè‡ªåŠ¨æŠŠå®ƒè¿½åŠ åˆ°å†å²é‡Œ
        return {
            "answer": response.content,
            "messages": [AIMessage(content=response.content)] 
        }
    except Exception as e:
        logger.error(f"âŒ Writing failed: {e}")
        return {"answer": "Error generating answer."}
import json
import yaml
from typing import Dict, Any, List

from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.documents import Document
from langchain_qdrant import QdrantVectorStore

from config.settings import settings
from core.llm import get_agent_llm, get_embeddings, get_extractor_llm
from core.qdrant import qdrant_manager
from core.search import search_tool
from core.pdf_loader import load_pdf_as_images
from graph.research.state import ResearchState
from utils.logger import logger

# --- è¾…åŠ©å‡½æ•°: åŠ è½½ Research Prompt ---
def load_prompts():
    prompt_path = settings.PROMPTS_DIR / "research.yaml"
    with open(prompt_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

PROMPTS = load_prompts()

# ==========================================
# Node 1: æ„å›¾è·¯ç”±èŠ‚ç‚¹ (Router)
# ==========================================
def router_node(state: ResearchState) -> Dict[str, Any]:
    """
    åˆ†æç”¨æˆ·æ„å›¾ï¼šæ˜¯åªæŸ¥æœ¬åœ°çŸ¥è¯†åº“ï¼Œè¿˜æ˜¯éœ€è¦è”ç½‘ï¼Ÿ
    """
    logger.info("ğŸš¦ Processing Node: Router")

    if not state.get("allow_web_search", True):
        logger.info("   ğŸš« Web search disabled by user. Forcing local retrieval.")
        return {"router_decision": "retrieve"}
    
    question = state["question"]
    
    llm = get_agent_llm(temperature=0) # å†³ç­–éœ€è¦ç¨³å®š
    prompt_cfg = PROMPTS["router"]
    
    messages = [
        SystemMessage(content=prompt_cfg["system"]),
        HumanMessage(content=prompt_cfg["user"].format(question=question))
    ]
    
    try:
        response = llm.invoke(messages)
        content = response.content.replace("```json", "").replace("```", "").strip()
        decision_json = json.loads(content)
        decision = decision_json.get("decision", "web_search") # é»˜è®¤è”ç½‘ï¼Œæ¯”è¾ƒç¨³å¦¥
        
        logger.info(f"   ğŸ‘‰ Decision: {decision}")
        return {"router_decision": decision}
        
    except Exception as e:
        logger.error(f"âŒ Router failed: {e}. Fallback to web_search.")
        return {"router_decision": "web_search"}

# ==========================================
# Node 2: æœ¬åœ°æ£€ç´¢èŠ‚ç‚¹ (Retriever) + ä¸Šä¼ å¤„ç†
# ==========================================
def retrieve_node(state: ResearchState) -> Dict[str, Any]:
    """
    1. å¤„ç†ä¸Šä¼ çš„ PDF (å¦‚æœæœ‰) -> è½¬æ¢ä¸º Text
    2. ä» Qdrant æ£€ç´¢ç›¸å…³æ–‡æ¡£
    """
    logger.info("ğŸ” Processing Node: Retriever & Processor")
    question = state["question"]
    top_k = state.get("top_k", 5)
    uploaded_path = state.get("uploaded_file_path")
    
    context_docs = []

    # --- A. å¤„ç†ä¸´æ—¶ä¸Šä¼ çš„æ–‡ä»¶ ---
    if uploaded_path:
        try:
            logger.info(f"   ğŸ“„ Processing Uploaded PDF: {uploaded_path}")
            # 1. è½¬å›¾ç‰‡
            images = load_pdf_as_images(uploaded_path, max_pages=100)
            
            # 2. è§†è§‰æ¨¡å‹æå–æ‘˜è¦
            llm = get_extractor_llm()
            user_content = [
                {"type": "text", "text": "Please analyze these images of a research paper. Provide a comprehensive summary including: Title, Authors, Key Contributions, Methodology, Main Results, and Limitations. This summary will be used to compare with other papers."}
            ]
            for img_b64 in images:
                user_content.append({
                    "type": "image_url", 
                    "image_url": {"url": f"data:image/png;base64,{img_b64}"}
                })
            
            msg = [HumanMessage(content=user_content)]
            response = llm.invoke(msg)
            
            # 3. å°è£…ä¸º Document
            upload_doc = Document(
                page_content=f"--- [UPLOADED TARGET PAPER] ---\n{response.content}",
                metadata={"title": "Uploaded User Paper", "source": "uploaded_file", "year": "Current"}
            )
            context_docs.append(upload_doc)
            logger.info("   âœ… Uploaded file processed and added to context.")
            
        except Exception as e:
            logger.error(f"   âŒ Failed to process upload: {e}")

    # --- B. Qdrant æ£€ç´¢ ---
    try:
        client = qdrant_manager.client
        embedding_model = get_embeddings()
        
        vector_store = QdrantVectorStore(
            client=client,
            collection_name=settings.QDRANT_COLLECTION_NAME,
            embedding=embedding_model
        )
        
        # æ£€ç´¢ Top K
        docs = vector_store.similarity_search(question, k=top_k)
        logger.info(f"   âœ… Retrieved {len(docs)} documents from DB.")
        
        context_docs.extend(docs)
        
    except Exception as e:
        logger.error(f"âŒ Retrieval failed: {e}")
    
    return {"context": context_docs}

# ==========================================
# Node 3: è”ç½‘æœç´¢èŠ‚ç‚¹ (Web Search)
# ==========================================
def web_search_node(state: ResearchState) -> Dict[str, Any]:
    """
    ç”Ÿæˆå…³é”®è¯ -> è”ç½‘æœç´¢ -> å°è£…ä¸º Document
    """
    logger.info("ğŸŒ Processing Node: Web Search")
    question = state["question"]
    existing_context = state.get("context", [])
    
    llm = get_agent_llm()
    
    # 1. ç”Ÿæˆæœç´¢è¯
    prompt_cfg = PROMPTS["generate_search_query"]
    messages = [
        SystemMessage(content=prompt_cfg["system"]),
        HumanMessage(content=prompt_cfg["user"].format(question=question))
    ]
    query_res = llm.invoke(messages).content.strip()
    # ç®€å•å¤„ç†ï¼šå‡è®¾ LLM è¿”å›çš„æ˜¯é€—å·åˆ†éš”çš„å…³é”®è¯
    queries = [q.strip() for q in query_res.split(",")]
    
    logger.info(f"   ğŸ” Generated Queries: {queries}")
    
    # 2. æ‰§è¡Œæœç´¢ (åªæœç¬¬ä¸€ä¸ªè¯ï¼Œæˆ–è€…å¹¶å‘æœ)
    # ä¸ºäº†æ¼”ç¤ºç®€å•ï¼Œæˆ‘ä»¬åªç”¨ç¬¬ä¸€ä¸ªå…³é”®è¯å»æœ
    search_query = queries[0]
    search_result_str = search_tool.search(search_query)
    
    # 3. å°†æœç´¢ç»“æœå°è£…æˆ Document å¯¹è±¡ï¼Œä»¥ä¾¿å’Œ Qdrant ç»“æœæ ¼å¼ç»Ÿä¸€
    web_doc = Document(
        page_content=search_result_str,
        metadata={"source": "web_search", "query": search_query}
    )
    
    # 4. è¿½åŠ åˆ°ç°æœ‰ Context
    return {
        "context": existing_context + [web_doc], # åˆå¹¶
        "search_queries": queries
    }

# ==========================================
# Node 4: æ’°å†™èŠ‚ç‚¹ (Writer)
# ==========================================
def writer_node(state: ResearchState) -> Dict[str, Any]:
    """
    è¯»å– Context -> ç”Ÿæˆæœ€ç»ˆå›ç­”
    """
    logger.info("âœï¸ Processing Node: Writer")
    temperature = state.get("temperature", 0.5)
    question = state["question"]
    context_docs = state.get("context", [])
    messages = state.get("messages", [])
    
    if not context_docs:
        return {"answer": "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç›¸å…³èµ„æ–™ï¼Œæ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚"}
    
    # 1. æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
    context_str = ""
    for i, doc in enumerate(context_docs):
        source = doc.metadata.get("title", "Web Search")
        venue = doc.metadata.get("venue", "")
        year = doc.metadata.get("year", "")
        # æ ‡è®°ä¸Šä¼ çš„æ–‡ä»¶
        if doc.metadata.get("source") == "uploaded_file":
            source = "[User Uploaded PDF]"
            
        context_str += f"\n--- Reference {i+1} ({source}) ---\n{doc.page_content}\n"
    
    # 2. æ ¼å¼åŒ–å†å²æ¶ˆæ¯
    history_str = ""
    recent_history = messages[:-1] # ä¸åŒ…å«å½“å‰æœ€æ–°çš„è¿™æ¡é—®é¢˜
    for msg in recent_history:
        role = "User" if isinstance(msg, HumanMessage) else "Assistant"
        history_str += f"{role}: {msg.content}\n"
    
    # 3. è°ƒç”¨ LLM
    llm = get_agent_llm(temperature=temperature) 
    prompt_cfg = PROMPTS["write_review"]
    system_msg = prompt_cfg["system"].format(
        context=context_str,
        chat_history=history_str, 
        question=question
    )
    
    msg_payload = [
        SystemMessage(content=system_msg),
        HumanMessage(content=question)
    ]
    
    try:
        response = llm.invoke(msg_payload)
        logger.info("   âœ… Answer generated.")
        
        # å¢åŠ å‚è€ƒæ–‡çŒ®
        ref_section = "\n\n---\n### ğŸ“š References\n\n"
        for i , doc in enumerate(context_docs):
            meta = doc.metadata
            index = i+1
            
            if meta.get("source") == "uploaded_file":
                ref_section += f"**[{index}]** ğŸ“‚ **User Uploaded PDF**: *Analyzed Content*\n\n"
            elif meta.get("source") == "web_search":
                query = meta.get("query", "General Search")
                ref_section += f"**[{index}]** ğŸŒ **Web Search**: *{query}* (Content from Tavily)\n\n"
            else:
                # è®ºæ–‡æ¥æº
                title = meta.get("title", "Unknown Title")
                venue = meta.get("venue", "Unknown Venue")
                year = meta.get("year", "N/A")
                authors = meta.get("authors", [])
                
                auth_str = "Unknown Authors"
                if isinstance(authors, list) and len(authors) > 0:
                    auth_str = ", ".join(authors[:2])
                    if len(authors) > 2: auth_str += " et al."
                
                ref_section += f"**[{index}]** ğŸ“„ **{title}**\n"
                ref_section += f"   - *{auth_str}* | {venue}, {year}\n\n"
                
        final_content = response.content + ref_section
        
        return {
            "answer": final_content,
            "messages": [AIMessage(content=final_content)] 
        }
    except Exception as e:
        logger.error(f"âŒ Writing failed: {e}")
        return {"answer": "Error generating answer."}